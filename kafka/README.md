## Kafka Producers, Streams, and Consumers:

The Kafka architecture consists of a producer, three brokers, two Kafka streams applications, and several consumers that pull directly from the topic that the producer pushes to as well as from the topics created by the Kafka streams applications. The brokers are created when installing Kafka with pegasus.

Kafka topics:
1. fake_iot -> this is the topic that the producer outputs to
2. cumulativesum -> this is the topic generated by JsonConnectSum.java application
3. movingavg -> one of the branch topics generated by the MovingAverageAnomaly.java application
4. outage -> the second of the branch topics generated by the MovingAverageAnomaly.java application
5. theft -> the final of the branch topics generated by the MovingAverageAnomaly.java application

The Kafka topics should be created BEFORE running the producer or any of the consumers.

Create all topics:
```
$ cd bashScripts
$ ./createPipelineTopics.sh
```

Now run the pipeline bash scripts in this order:
1. ./pipelineProducer.sh
2.
```
$ peg ssh kafka 2
$ java -jar /home/ubuntu/movingavgstream.jar
(this is necessary b/c ctrl-c needs to shut down the application
for the reset in deletePipelineTopics to be effective)
```
3.
```
$ peg ssh kafka 2
$ java -jar /home/ubuntu/jsonsum.jar
(!! This shouldn't produce any output except for initial connection to kafka brokers !!)
```

4. ./pipelineTimeSeriesConsumer.sh
5. ./pipelineMovingAvgConsumer.sh
6. ./pipelineOutageConsumer.sh
7. ./pipelineTheftConsumer.sh
8. ./pipelineSumRedisConsumer.sh

By doing so, will being putting data into the Cassandra and Redis tables.
